{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CAS Deep Learning - Computer Vision mit Deep Learning (Part 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c277da4aa8dbb809e84745f1169f9536",
     "grade": false,
     "grade_id": "header",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "try:\n",
    "    import jupyter_black\n",
    "\n",
    "    jupyter_black.load()\n",
    "except:\n",
    "    print(\"black not installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8053a41bf33fae8f19ac51281f4d6297",
     "grade": false,
     "grade_id": "title",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# Image Classification - Project\n",
    "\n",
    "## Learning Goals\n",
    "\n",
    "- Learn how to model an image classification task\n",
    "- Learn how to systematically implement data prep, model, training, and evaliation\n",
    "- Learn how to check and verify the implementation\n",
    "- Learn how to incorporate boilerplate code from [torchvision](https://pytorch.org/vision/0.9/index.html) and  [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0b3592875de63679346c48263ac7ec03",
     "grade": false,
     "grade_id": "cell-a9856bbf98ea570f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Setup\n",
    "\n",
    "We setup our environment and data save / load paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bce791b7343c4ea54a5e764d57d57d7f",
     "grade": false,
     "grade_id": "imports",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e43ad09da2664c93d9494ef703961b80",
     "grade": false,
     "grade_id": "cell-964e127b4b508779",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Mount your google drive to store data and results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9a7a09d3adf20b74231fda01be8637cc",
     "grade": false,
     "grade_id": "cell-d573733e6ec05a88",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "print(f\"In colab: {IN_COLAB}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the following paths if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    DATA_PATH = Path(\"/content/drive/MyDrive/cas-dl-module-compvis-part1\")\n",
    "else:\n",
    "    DATA_PATH = Path(\"../data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install packages not in base Colab environment and required by you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    os.system(\"pip install torchshow torchinfo pytorch-lightning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Change the following parameter to false**: `DEV_RUNS=False`\n",
    "\n",
    "Otherwise model training code will only run a small number of times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the following parameter to False, otherwise model training is only conducted for a small number of steps to test the code\n",
    "DEV_RUNS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c04d726b97a5c547f986d9bbe51c84ff",
     "grade": false,
     "grade_id": "projects",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Project Selection\n",
    "\n",
    "Choose one of the following projects to work on. \n",
    "\n",
    "Choose Cats vs Dogs if you want to mainly click through the template (some modificaitons are required!). All other datasets require more substantial adaptations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cats vs Dogs\n",
    "\n",
    "**Goal**: Develop a model to classify images of cats and dogs. The dataset is designed to facilitate the identification of these animals from images.\n",
    "\n",
    "**Approach**: Create a Convolutional Neural Network (CNN) to classify the images into two categories: cats and dogs. Experiment with various CNN architectures and techniques to determine the most effective method. Use data augmentation techniques to handle variations in pose, lighting, and background. Ensure the model generalizes well by using cross-validation and monitoring for overfitting.\n",
    "\n",
    "**Dataset**: The dataset contains 25,000 images, with approximately 12,500 images per class (cats and dogs). Each image varies in size and resolution. The data is provided by Microsoft as part of their Kaggle competition.\n",
    "\n",
    "[Source](https://www.microsoft.com/en-us/download/details.aspx?id=54765)\n",
    "\n",
    "![Dog](dog.jpg)\n",
    "![Cat](cat.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concrete Crack Detection\n",
    "\n",
    "**Goal**: Develop a model to classify concrete images as having cracks or not. The dataset is designed to facilitate the identification of structural issues in concrete buildings.\n",
    "\n",
    "**Approach**: Create a Convolutional Neural Network (CNN) to classify the images into negative (no crack) and positive (crack) categories. Experiment with various CNN architectures and techniques to determine the most effective method. Use image processing techniques to handle variations in surface finish and illumination. Ensure the model generalizes well by using cross-validation and monitoring for overfitting.\n",
    "\n",
    "**Dataset**: The dataset contains 40,000 images, with 20,000 images per class (negative and positive). Each image is 227 x 227 pixels with RGB channels. The data is collected from 458 high-resolution images (4032 x 3024 pixels) from various METU Campus Buildings. No data augmentation such as random rotation or flipping is applied.\n",
    "\n",
    "[Source](https://data.mendeley.com/datasets/5y9wdsg2zt/2)\n",
    "\n",
    "![Crack](crack_example.jpg)\n",
    "![No Crack](crack_negative.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scene Classification\n",
    "\n",
    "**Goal**: Develop a model to classify natural scene images into one of six categories. The dataset aims to facilitate the recognition of various natural scenes from around the world.\n",
    "\n",
    "**Approach**: Design a Convolutional Neural Network (CNN) to classify images into six categories: buildings, forest, glacier, mountain, sea, and street. Test different CNN architectures to find the best performing model. Apply data augmentation techniques to improve generalization. Separate the data into training, testing, and prediction sets to evaluate model performance effectively.\n",
    "\n",
    "**Dataset**: The dataset contains around 25,000 images of size 150 x 150 pixels, distributed across six categories. The data is separated into training (14,000 images), testing (3,000 images), and prediction (7,000 images) sets.\n",
    "\n",
    "[Source](https://www.kaggle.com/datasets/puneet6060/intel-image-classification?resource=download)\n",
    "\n",
    "\n",
    "![Builings](natural_scenes_buildings.jpg)\n",
    "![Forest](natural_scenes_forest.jpg)\n",
    "![Glacier](natural_scenes_glacier.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Satellite Land Cover Classification\n",
    "\n",
    "**Goal**: Develop a model to classify satellite images into different land cover types. The dataset contains images of 10 different classes and aims to support land use and land cover classification tasks.\n",
    "\n",
    "**Approach**: Develop Convolutional Neural Networks (CNNs) to model the satellite image data. Experiment with different CNN architectures to identify the best performing model. Compare pre-trained models with those trained from scratch. Use data augmentation techniques to enhance model generalization. Given the relatively small dataset, pay attention to overfitting and compare models robustly.\n",
    "\n",
    "**Dataset**: The dataset consists of 27,000 RGB images categorized into 10 classes. The dataset is available in two formats: one in RGB and another with 13 spectral bands. Use the RGB dataset for this project.\n",
    "\n",
    "[Source](https://github.com/phelber/eurosat)\n",
    "\n",
    "![Crop](sat_crop.jpg)\n",
    "![Forest](sat_forest.jpg)\n",
    "![Highway](sat_highway.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose your own dataset!\n",
    "\n",
    "Feel free to choose your own dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overall Approach\n",
    "\n",
    "Inspired by [A Recipe for Training Neural Networks by Andrej Karpathy](https://karpathy.github.io/2019/04/25/recipe/)\n",
    "\n",
    "For your chosen dataset. Do the following:\n",
    "\n",
    "\n",
    "## 1) Data Preparation & Data Inspection\n",
    "\n",
    "- Download the data\n",
    "- Inspect the data formats\n",
    "- Build a `torch.utils.data.Dataset`\n",
    "    - define training, validation and test sets\n",
    "- Implement a `torch.utils.data.DataLoader'\n",
    "- Inspect the data:\n",
    "    - Look at samples\n",
    "    - Inspect the label distribution\n",
    "\n",
    "## 2) Baselines\n",
    "\n",
    "- Implement a small CNN\n",
    "- Learn input-independent baseline (provide only labels but random noise as input)\n",
    "- Overfitt CNN on one batch\n",
    "- Inspect pre-processing\n",
    "  \n",
    "## 3) (Over)fit\n",
    "- Build a large(er) architecture (pre-trained or self-implemented)\n",
    "- Train a high-performing model with respect to training set\n",
    "  \n",
    "## 4) Regularize\n",
    "- Is it beneficial to collect more data?\n",
    "- Data Augmentation\n",
    "- Early Stopping on Validation Set\n",
    "- Weight Decay\n",
    "\n",
    "## 5) Hyper-Parameter Tuning\n",
    "- Define HPs and parameterise architecture\n",
    "- do grid- or random search over HP grids\n",
    "\n",
    "## 6) Squeeze out the juice\n",
    "-  Ensembling\n",
    "-  Longer training\n",
    "-  Special techniques: AdamW optimizer, fancy data augmentation, label smoothing, stochastic depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 - Data Preparation & Data Inspection\n",
    "\n",
    "**_This step is critical. I like to spend copious amount of time (measured in units of hours) scanning through thousands of examples, understanding their distribution and looking for patterns. (A Karpathy)_**\n",
    "\n",
    "- Download the data\n",
    "- Inspect the data formats and file organization\n",
    "- Remove corrupt data\n",
    "- Build a [torch.utils.data.Dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset)\n",
    "    - define training, validation and test sets\n",
    "- Implement a [torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)\n",
    "- Inspect the data:\n",
    "    - Look at samples\n",
    "    - Inspect the label distribution\n",
    "\n",
    "\n",
    "Important information about how to define a dataset can be found here: [https://pytorch.org/docs/stable/data.htm](https://pytorch.org/docs/stable/data.htm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the Data\n",
    "\n",
    "The following code snippets help you get startet with the data. It may take a while to download though...\n",
    "\n",
    "Load the functions and jump to the dataset that you have chosen!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def download_and_extract_zip(url: str, save_path: Path, extract_path: Path):\n",
    "    \"\"\"\n",
    "    Downloads a ZIP file from a given URL and extracts its contents to a specified directory.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the ZIP file to download.\n",
    "        save_path (Path): The path where the downloaded ZIP file will be saved.\n",
    "        extract_path (Path): The directory where the ZIP file will be extracted.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import requests\n",
    "    import zipfile\n",
    "\n",
    "    # Make sure the directory exists\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "    if not save_path.exists():\n",
    "        # Download the file\n",
    "        response = requests.get(url, stream=True)\n",
    "        with open(save_path, \"wb\") as file:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                _ = file.write(chunk)\n",
    "\n",
    "        print(f\"File downloaded and saved to {save_path}\")\n",
    "\n",
    "    if not extract_path.exists():\n",
    "        # Unzip the file\n",
    "        with zipfile.ZipFile(save_path, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(extract_path)\n",
    "\n",
    "        print(f\"File extracted to {extract_path}\")\n",
    "\n",
    "\n",
    "def download_from_gdrive_and_extract_zip(\n",
    "    file_id: str, save_path: Path, extract_path: Path\n",
    "):\n",
    "    \"\"\"\n",
    "    Downloads a ZIP file from Google Drive using its file ID and extracts its contents to a specified directory.\n",
    "\n",
    "    Args:\n",
    "        file_id (str): The Google Drive file ID of the ZIP file to download.\n",
    "        save_path (Path): The path where the downloaded ZIP file will be saved.\n",
    "        extract_path (Path): The directory where the ZIP file will be extracted.\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import gdown\n",
    "    import zipfile\n",
    "\n",
    "    url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "    if not save_path.exists():\n",
    "        gdown.download(url, str(save_path), quiet=False)\n",
    "        print(f\"File downloaded and saved to {save_path}\")\n",
    "\n",
    "    if not extract_path.exists():\n",
    "        # Unzip the file\n",
    "        with zipfile.ZipFile(save_path, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(extract_path)\n",
    "\n",
    "        print(f\"File extracted to {extract_path}\")\n",
    "\n",
    "\n",
    "def delete_bad_file(file_path: Path):\n",
    "    \"\"\"\n",
    "    Deletes a specified file if it exists.\n",
    "\n",
    "    Args:\n",
    "        file_path (Path): The path of the file to be deleted.\n",
    "    \"\"\"\n",
    "    import os\n",
    "\n",
    "    # Check if file exists before trying to delete it\n",
    "    if os.path.exists(file_path):\n",
    "        os.remove(file_path)\n",
    "        print(f\"{file_path} has been deleted\")\n",
    "    else:\n",
    "        print(f\"{file_path} does not exist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cats vs Dogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_and_extract_zip(\n",
    "    url=\"https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_5340.zip\",\n",
    "    save_path=DATA_PATH.joinpath(\"cats_vs_dogs.zip\"),\n",
    "    extract_path=DATA_PATH.joinpath(\"cats_vs_dogs/\"),\n",
    ")\n",
    "\n",
    "bad_files = [\n",
    "    DATA_PATH.joinpath(\"cats_vs_dogs\") / \"PetImages\" / \"Cat\" / \"666.jpg\",\n",
    "    DATA_PATH.joinpath(\"cats_vs_dogs\") / \"PetImages\" / \"Dog\" / \"11702.jpg\",\n",
    "]\n",
    "\n",
    "\n",
    "for bad_file in bad_files:\n",
    "    delete_bad_file(bad_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case you chose EuroSat Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_and_extract_zip(\n",
    "    url=\"https://zenodo.org/records/7711810/files/EuroSAT_RGB.zip?download=1\",\n",
    "    save_path=DATA_PATH.joinpath(\"EuroSAT_RGB.zip\"),\n",
    "    extract_path=DATA_PATH.joinpath(\"EuroSAT_RGB/\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concrete Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_and_extract_zip(\n",
    "    url=\"https://prod-dcd-datasets-cache-zipfiles.s3.eu-west-1.amazonaws.com/5y9wdsg2zt-2.zip\",\n",
    "    save_path=DATA_PATH.joinpath(\"concrete.zip\"),\n",
    "    extract_path=DATA_PATH.joinpath(\"concrete/\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scene classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_from_gdrive_and_extract_zip(\n",
    "    file_id=\"1Bx3R56VBONS-x91wCDU6KX3xqPoJoH9P\",\n",
    "    save_path=DATA_PATH / \"scene_classification.zip\",\n",
    "    extract_path=DATA_PATH.joinpath(\"scene_classification/\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have your own dataset it you could organize the images into class specific folders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Data Format & Organization, Build Dataset and Loader\n",
    "\n",
    "We need to figure out how the data is organized. Particularly, how the data is labelled, to correctly define it with a [torch.utils.data.Dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset).\n",
    "\n",
    "First you should look at the data / folder structure of the downloaded data.\n",
    "\n",
    "Once you have figured out how the data is organized we can build a [torch.utils.data.Dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset). A [torch.utils.data.Dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) allows for iterating over your dataset, returning tuples of images and labels at each iteration.\n",
    "\n",
    "**Note: To evaluate and select models in a later stage, we already create a training, validation and a test dataset.**\n",
    "\n",
    "Adapt the following code if necessary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Tuple, List\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, Subset\n",
    "\n",
    "\n",
    "def load_image_paths_and_labels(image_dir: str) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Load image paths and corresponding labels.\n",
    "\n",
    "    Args:\n",
    "        image_dir: Directory with all the images.\n",
    "\n",
    "    Returns:\n",
    "        A tuple of (image paths, labels).\n",
    "    \"\"\"\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    classes = os.listdir(image_dir)\n",
    "    image_extensions = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".gif\"}\n",
    "\n",
    "    for label in classes:\n",
    "        class_dir = os.path.join(image_dir, label)\n",
    "        for img_name in os.listdir(class_dir):\n",
    "            img_path = os.path.join(class_dir, img_name)\n",
    "            if any(img_path.lower().endswith(ext) for ext in image_extensions):\n",
    "                image_paths.append(img_path)\n",
    "                labels.append(label)\n",
    "\n",
    "    return image_paths, labels\n",
    "\n",
    "\n",
    "def create_train_test_split(\n",
    "    image_paths: List[str],\n",
    "    labels: List[str],\n",
    "    test_size: float = 0.2,\n",
    "    random_state: int = None,\n",
    ") -> Tuple[List[str], List[str], List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Create stratified train and test splits.\n",
    "\n",
    "    Args:\n",
    "        image_paths: List of image paths.\n",
    "        labels: List of labels.\n",
    "        test_size: The proportion of the dataset to include in the test split.\n",
    "        random_state: Controls the shuffling applied to the data before applying the split.\n",
    "\n",
    "    Returns:\n",
    "        train_image_paths, test_image_paths, train_labels, test_labels\n",
    "    \"\"\"\n",
    "    train_image_paths, test_image_paths, train_labels, test_labels = train_test_split(\n",
    "        image_paths,\n",
    "        labels,\n",
    "        stratify=labels,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "\n",
    "    return train_image_paths, test_image_paths, train_labels, test_labels\n",
    "\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_paths: List[str],\n",
    "        labels: List[str],\n",
    "        transform: Callable | None = None,\n",
    "        classes: List[str] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_paths: List of image paths.\n",
    "            labels: List of labels.\n",
    "            transform: Optional transform to be applied on a sample.\n",
    "            classes: List of class names.\n",
    "        \"\"\"\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.classes = classes if classes is not None else sorted(set(labels))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            idx: Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image, label) where label is the image classification.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            image_path = self.image_paths[idx]\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            label = self.labels[idx]\n",
    "            label_num = self.classes.index(label)\n",
    "\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image, label_num\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image at index {idx}: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "# Specify your image root path\n",
    "image_root_path = DATA_PATH.joinpath(\"cats_vs_dogs/PetImages\")\n",
    "image_paths, labels = load_image_paths_and_labels(image_root_path)\n",
    "\n",
    "# Create Train, Validation and Test Splits\n",
    "train_image_paths, test_image_paths, train_labels, test_labels = (\n",
    "    create_train_test_split(image_paths, labels, test_size=0.2, random_state=123)\n",
    ")\n",
    "train_image_paths, validation_image_paths, train_labels, validation_labels = (\n",
    "    create_train_test_split(\n",
    "        train_image_paths, train_labels, test_size=0.1, random_state=123\n",
    "    )\n",
    ")\n",
    "\n",
    "# Specify transformations\n",
    "train_transform = None\n",
    "test_transform = None\n",
    "validation_transform = None\n",
    "\n",
    "ds_train = ImageDataset(train_image_paths, train_labels, transform=train_transform)\n",
    "ds_validation = ImageDataset(\n",
    "    validation_image_paths, validation_labels, transform=validation_transform\n",
    ")\n",
    "ds_test = ImageDataset(test_image_paths, test_labels, transform=test_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a693d70e9b56933974aff4da26c8b000",
     "grade": false,
     "grade_id": "cell-974feaab57d71ef1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question**: What is the role of: `label_num = self.classes.index(label)`?\n",
    "\n",
    "**Question**: Why do we (often)  need a training, validation and a testset?\n",
    "\n",
    "**Question**: Why do we need transformations. And why do we need different ones for train and test?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e2ed81eb45dcde76dd79b913c96b9677",
     "grade": true,
     "grade_id": "cell-3df732b70f3385a9",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we test the `Dataset` object by getting and visualising a sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchshow as ts\n",
    "\n",
    "image, label = ds_train[0]\n",
    "ts.show(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For model training we need to batch examples. Thats why we need to define a [torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader).\n",
    "\n",
    "We also need to convert the images to [Tensors](https://pytorch.org/docs/stable/tensors.html). We can use the `transform` parameter of our `ImageDataset` class to specify transformations using [torchvision.transforms](https://pytorch.org/vision/0.9/transforms.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# Define a simple transformation\n",
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create the dataset from the training data and a dataloader\n",
    "ds_train = ImageDataset(train_image_paths, train_labels, transform=train_transform)\n",
    "dataloader_train = DataLoader(ds_train, batch_size=16, shuffle=True)\n",
    "\n",
    "images, labels = next(iter(dataloader_train))\n",
    "\n",
    "ts.show(images)\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "759bc6b2c68fac33c539dca022611801",
     "grade": false,
     "grade_id": "cell-8385aee74294e0fc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question**: What does `shuffle=True` achieve? Why is it recommended?\n",
    "\n",
    "**Question**: Why do we use `torch.manual_seed(123)`? What does it do in this example?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d366c2247b08b2559894f8891a32638d",
     "grade": true,
     "grade_id": "cell-b27f3f9aa63b1694",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the Data\n",
    "\n",
    "Now you can use the `ImageDataset` or `DataLoader` objects to insepct the dataset. \n",
    "\n",
    "- **Initial Step**: Avoid touching neural net code initially; focus on inspecting the data thoroughly.\n",
    "- **Time Investment**: Spend hours scanning thousands of examples to understand their distribution and look for patterns.\n",
    "- **Identify Issues**: Look for duplicate examples, corrupted images/labels, data imbalances, and biases.\n",
    "- **Classify Process**: Pay attention to how you classify the data to inform the architecture exploration.\n",
    "- **Feature Analysis**: Determine if local features or global context is needed.\n",
    "- **Variation Analysis**: Assess the variation in the data, identify spurious variations for preprocessing.\n",
    "- **Spatial Consideration**: Evaluate if spatial position matters or if averaging it out is beneficial.\n",
    "- **Detail and Downsampling**: Consider the importance of detail and the feasibility of downsampling images.\n",
    "- **Label Noise**: Assess the noise level in the labels.\n",
    "- **Understand Predictions**: Use network (mis)predictions to understand inconsistencies and data issues (at a later stage!).\n",
    "- **Quantitative Analysis**: Write simple code to search, filter, and sort data by various attributes.\n",
    "- **Visualize Distributions**: Visualize distributions and outliers to uncover bugs in data quality or preprocessing.\n",
    "\n",
    "For now do at least the following:\n",
    "- what is the class distribution? Use, for example: [numpy.unique](https://numpy.org/doc/stable/reference/generated/numpy.unique.html).\n",
    "- how difficult do you think is the problem?\n",
    "- are there any obvious issues with the data?\n",
    "- do the labels seem accurate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fefa064fb8714b9f7c3f50c0e9a6ae8e",
     "grade": true,
     "grade_id": "cell-556797ba20290ab9",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 - Implement Baselines\n",
    "\n",
    "In this step we want to implement a training pipeline and evaluate simple baselines to get a feeling for the problem and to test and verify if the pipeline works.\n",
    "\n",
    "**Reproducibility**\n",
    "\n",
    "- Fix random seed: Always use a fixed random seed to ensure consistent outcomes in repeated runs.\n",
    "\n",
    "**Simplification and Initialization**\n",
    "\n",
    "- Simplify: Disable unnecessary features like data augmentation initially.\n",
    "- Verify loss at initialization: Ensure loss starts at the expected value.\n",
    "\n",
    "**Baselines and Metrics**\n",
    "\n",
    "- Human baseline: Compare model metrics to human-interpretable metrics (e.g., accuracy).\n",
    "- Input-independent baseline: Train a baseline model with zeroed inputs and compare it to a variant with normal data. There should be a clear difference!\n",
    "\n",
    "**Overfitting and Visualization**\n",
    "\n",
    "- Overfit one batch: Overfit a single batch to verify the model can reach the minimum loss.\n",
    "- Verify decreasing training loss: Ensure training loss decreases when model capacity increases.\n",
    "- Visualize before the net: Visualize data immediately before feeding it to the network to catch preprocessing issues.\n",
    "- Visualize prediction dynamics: Track model predictions on a fixed test batch during training to understand training progression.\n",
    "\n",
    "**Evaluation**\n",
    "\n",
    "- Add significant digits to your eval: Evaluate on the entire test set for accuracy.\n",
    "- Visualize: Visualize model inputs and outputs to ensure correctness.\n",
    "\n",
    "**Additional Tips**\n",
    "\n",
    "- Verify simplifications: Simplify initial setup by turning off data augmentation and complex features to reduce bugs.\n",
    "\n",
    "We will address some of the steps above. Feel free to do more!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducibility\n",
    "\n",
    "Pytorch lightning provides a function to set random seeds of different modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as ptl\n",
    "\n",
    "ptl.seed_everything(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple DataLoader\n",
    "\n",
    "Implement a simple dataloader without fancy transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# Define a simple transformation\n",
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create the dataset from the training data and a dataloader\n",
    "ds_train = ImageDataset(train_image_paths, train_labels, transform=train_transform)\n",
    "dataloader_train = DataLoader(ds_train, batch_size=16, shuffle=True)\n",
    "\n",
    "images, labels = next(iter(dataloader_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Model\n",
    "\n",
    "Start with a simple model that is (most likely) correct and should be able to learn something (quickly).\n",
    "\n",
    "For example you could implement the following architecture.\n",
    "\n",
    "- Input Shape: (3, **height**, **width**)\n",
    "- Convolution: 16 Filters, Kernel-Size 5x5\n",
    "- Pooling: Stride 2, Kernel-Size 2\n",
    "- Convolution: 32 Filter, Kernel-Size 5x5\n",
    "- Global Average Pooling\n",
    "- FC: 2 neurons (**number of classes**)\n",
    "\n",
    "Use `ReLU` activation after each convolution.\n",
    "\n",
    "Define a class which inherits from `torch.nn.Module`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "31aa19ac3f0df808ff4d983be2666796",
     "grade": true,
     "grade_id": "cell-282219b1ddf201ab",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import torchinfo\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class SmallCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, (5, 5))\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 5)\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = torch.flatten(x, 1)  # flatten all dimensions except batch\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        return x\n",
    "\n",
    "\n",
    "net = SmallCNN()\n",
    "\n",
    "print(net)\n",
    "print(torchinfo.summary(net, input_size=(1, 3, 64, 64)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1d95b535e1c5803f40dbf22b3252fd41",
     "grade": false,
     "grade_id": "cell-62ef301fb1d10cf0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question**: Briefly explain what happens with a data point during the forward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ccb0bf7b1f371bf50d7b8196a33e68c0",
     "grade": true,
     "grade_id": "cell-2df4e411b4d21da0",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a training Loop\n",
    "\n",
    "We use Pytorch-Lightning which greatly simplifys implementing boilerplate code such as  training loops.\n",
    "\n",
    "Tutorial here: https://lightning.ai/pages/community/tutorial/step-by-step-walk-through-of-pytorch-lightning/\n",
    "\n",
    "We also include additional metrics from [torchmetrics](https://lightning.ai/docs/torchmetrics/stable/) to easily log and calculate accuracy.  Adapt `task=\"binary\"` if necessary!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torchmetrics\n",
    "\n",
    "\n",
    "class Classifier(pl.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.accuracy = torchmetrics.Accuracy(task=\"binary\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.loss_fn(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "        # Update accuracy metric\n",
    "        acc = self.accuracy(preds, y)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        self.log(\"train_acc\", acc, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the following parameters accoring to your hardware. As you can see, this simplifies hardware switches greatly!\n",
    "\n",
    "We want to perform a functional check only. Train the model only for 10 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(\n",
    "    devices=1,\n",
    "    accelerator=\"cpu\",\n",
    "    precision=\"32\",\n",
    "    fast_dev_run=DEV_RUNS,\n",
    "    max_steps=10,\n",
    "    enable_checkpointing=False,\n",
    "    logger=False,\n",
    "    default_root_dir=DATA_PATH.joinpath(\"lightning_logs\"),\n",
    ")\n",
    "\n",
    "net = SmallCNN()\n",
    "model = Classifier(net)\n",
    "trainer.fit(model, train_dataloaders=dataloader_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Metrics:  {trainer.logged_metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Does the previous code display a useful information? If so, what does it say and why?\n",
    "\n",
    "**Question**: What is the loss at initialization? Does the value make sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6328f941cf9c333ecae4439d322f7088",
     "grade": true,
     "grade_id": "cell-64a035622a6a97bb",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train the model for longer to get a sense of the performance. Adjust the following code accordingly (increase the number of steps the model is training for)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(\n",
    "    devices=1,\n",
    "    accelerator=\"cpu\",\n",
    "    precision=\"32\",\n",
    "    max_steps=1,\n",
    "    enable_checkpointing=False,\n",
    "    logger=False,\n",
    "    default_root_dir=DATA_PATH.joinpath(\"lightning_logs\"),\n",
    ")\n",
    "\n",
    "net = SmallCNN()\n",
    "model = Classifier(net)\n",
    "trainer.fit(model, train_dataloaders=dataloader_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Metrics:  {trainer.logged_metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn Input independent Model\n",
    "\n",
    "\n",
    "Modify the `Dataset` class such that random images, e.g. white noise, is returned. The label remains unchanged. Then train a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d9b24a0d36f057b1d92e073cad417ba5",
     "grade": false,
     "grade_id": "cell-a1689e4300aaf490",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question**: What kind of loss do you expect if the model works?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "960afd90bd3959ab396f104d4fe37428",
     "grade": true,
     "grade_id": "cell-38d958f99b140ac7",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDatasetRandom(ImageDataset):\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            idx: Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image, label) where label is the image classification.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            image_path = self.image_paths[idx]\n",
    "            original_image = np.array(Image.open(image_path).convert(\"RGB\"))\n",
    "            image_shape = original_image.shape\n",
    "            random_image = Image.fromarray(\n",
    "                np.random.randint(0, 256, image_shape, dtype=np.uint8)\n",
    "            )\n",
    "\n",
    "            label = self.labels[idx]\n",
    "            label_num = self.classes.index(label)\n",
    "\n",
    "            if self.transform:\n",
    "                random_image = self.transform(random_image)\n",
    "            return random_image, label_num\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image at index {idx}: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "# Create the dataset and dataloader\n",
    "ds_train_random = ImageDatasetRandom(\n",
    "    train_image_paths, train_labels, transform=train_transform\n",
    ")\n",
    "dataloader_random = DataLoader(ds_train_random, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify your work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_random, label = ds_train_random[0]\n",
    "ts.show(image_random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(\n",
    "    devices=1,\n",
    "    accelerator=\"cpu\",\n",
    "    precision=\"32\",\n",
    "    fast_dev_run=DEV_RUNS,\n",
    "    max_steps=100,\n",
    "    enable_checkpointing=False,\n",
    "    logger=False,\n",
    "    default_root_dir=DATA_PATH.joinpath(\"lightning_logs\"),\n",
    ")\n",
    "\n",
    "net = SmallCNN()\n",
    "model = Classifier(net)\n",
    "trainer.fit(model, train_dataloaders=dataloader_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Metrics:  {trainer.logged_metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Were your expectations met? If not, why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "aabed4ba391dad60d5c1a86004f3de11",
     "grade": true,
     "grade_id": "cell-dcb7054f8b75db08",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfit on one Batch of Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "64f826f5e7bb804d539ce756c493a089",
     "grade": false,
     "grade_id": "cell-76e81732d77174f3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question**: What do you expect?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9106e3bb32a105b5c70ffffc2e2ba7f6",
     "grade": true,
     "grade_id": "cell-52aedf9dae498678",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(\n",
    "    devices=1,\n",
    "    accelerator=\"cpu\",\n",
    "    precision=\"32\",\n",
    "    fast_dev_run=DEV_RUNS,\n",
    "    max_steps=100,\n",
    "    enable_checkpointing=False,\n",
    "    logger=False,\n",
    "    default_root_dir=DATA_PATH.joinpath(\"lightning_logs\"),\n",
    "    # this option limits the training set to one batch, disables shuffel\n",
    "    overfit_batches=1.0,\n",
    ")\n",
    "\n",
    "net = SmallCNN()\n",
    "model = Classifier(net)\n",
    "\n",
    "ds_train = ImageDataset(train_image_paths, train_labels, transform=train_transform)\n",
    "dataloader_train = DataLoader(ds_train, batch_size=32, shuffle=False)\n",
    "trainer.fit(model, train_dataloaders=dataloader_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Metrics:  {trainer.logged_metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Did it work? Careful, there might be a bug!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "20582649a02d90950ed8c9d0fde3b76d",
     "grade": true,
     "grade_id": "cell-6d29fa78548ee969",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 - (Over)Fit\n",
    "\n",
    "In this step we try to drive the trainings-loss as low as possible.\n",
    "\n",
    "**Model Selection and Initialization**\n",
    "\n",
    "- Pick a proven model: Start with a simple, well-established architecture (e.g., ResNet-50 for image classification) rather than creating complex, custom models.\n",
    "- Use Adam optimizer: Begin with Adam and a learning rate of 3e-4 for its forgiving nature with hyperparameters (or the PyTorch default value).\n",
    "\n",
    "**Gradual Complexity**\n",
    "\n",
    "- Add complexity incrementally: Integrate multiple signals or features into your classifier one at a time, ensuring each addition improves performance.\n",
    "\n",
    "**Learning Rate Management**\n",
    "\n",
    "- Avoid default learning rate decay: Be cautious with repurposed code and learning rate decay schedules. Initially, disable learning rate decay and maintain a constant learning rate, tuning it later in the project.\n",
    "    \n",
    "\n",
    "You can do the following:\n",
    "- implement your own model\n",
    "- use a pre-defined model\n",
    "- use a pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1336a73cae07c21a44610fbda288504f",
     "grade": false,
     "grade_id": "cell-1f3c0de79c9f603b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## Pre-Trained Model\n",
    "\n",
    "In the following we will use a pre-trained model and adapt it to our dataset (transfer-learning).\n",
    "\n",
    "### Load Model\n",
    "\n",
    "Here we use a pre-trained model.  Read the doc here: [https://pytorch.org/vision/0.8/models.html](https://pytorch.org/vision/0.8/models.html).)\n",
    "\n",
    "**It is important to read how the data is pre-processed for a given pre-trained model. This should be consistent with how you pre-process the data.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d712c344f6689697736d6961f08e8078",
     "grade": true,
     "grade_id": "cell-39c22ca50688c3fe",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we adapt the output layer to match our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.fc = nn.Sequential(nn.Linear(512, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train the model. \n",
    "\n",
    "We also use a `logger` object to log the training process.\n",
    "\n",
    "Again: Adjust the parameters of the trainer class to your liking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "logger = TensorBoardLogger(\n",
    "    DATA_PATH.joinpath(\"lightning_logs\"), name=\"overfit_baseline1\"\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    devices=1,\n",
    "    accelerator=\"cpu\",\n",
    "    precision=\"32\",\n",
    "    max_steps=100,\n",
    "    fast_dev_run=DEV_RUNS,\n",
    "    enable_checkpointing=False,\n",
    "    logger=logger,\n",
    "    default_root_dir=DATA_PATH.joinpath(\"lightning_logs\"),\n",
    ")\n",
    "\n",
    "model = Classifier(net)\n",
    "\n",
    "ds_train = ImageDataset(train_image_paths, train_labels, transform=train_transform)\n",
    "dataloader_train = DataLoader(ds_train, batch_size=64, shuffle=False)\n",
    "trainer.fit(model, train_dataloaders=dataloader_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Metrics:  {trainer.logged_metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the tensorboard logs. This may not work in a container without opening tensorboard ports.\n",
    "\n",
    "(You would need to add the following options to docke run `-p 6006-6015:6006-6015`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir={DATA_PATH.joinpath(\"lightning_logs\")} --host localhost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: What can you observe? Describe what you see and propose any changes to the trainer class if you see opportunities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f67eb8a5776d637864531bd0227935c6",
     "grade": true,
     "grade_id": "cell-59807100000bb2a3",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feel Free to try a larger model\n",
    "\n",
    "Use a larger model and observe the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b8288ddc6590e1d317e6cf926c8deebd",
     "grade": true,
     "grade_id": "cell-44447682a47a144e",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 - Regularization\n",
    "\n",
    "Regularization is a process to deliberately limit a model's capacity in order to reduce overfitting and to improve generalization.\n",
    "\n",
    "**Data Collection and Augmentation**\n",
    "\n",
    "- Get more data: Collect additional real training data for the most effective regularization.\n",
    "- Data augmentation: Use more aggressive data augmentation techniques.\n",
    "- Creative augmentation: Explore simulation, hybrid methods, or GANs to expand datasets.\n",
    "\n",
    "**Model Initialization and Size**\n",
    "\n",
    "- Pretrain: Utilize pretrained networks when possible.\n",
    "- Smaller input dimensionality: Remove features with spurious signals and reduce image size if low-level details are not critical.\n",
    "- Smaller model size: Use domain knowledge to constrain and reduce the size of the network.\n",
    "\n",
    "**Regularization Techniques**\n",
    "\n",
    "- Decrease batch size: Smaller batch sizes can act as stronger regularizers due to batch normalization effects.\n",
    "- Add dropout: Use dropout (including dropout2d for ConvNets) sparingly.\n",
    "- Weight decay: Increase the weight decay penalty.\n",
    "- Early stopping: Stop training based on validation loss to avoid overfitting.\n",
    "\n",
    "**Model Complexity**\n",
    "\n",
    "- Try a larger model: Consider larger models for potentially better early-stopped performance, despite higher risk of eventual overfitting.\n",
    "\n",
    "  \n",
    "\n",
    "You can try the following techniques:\n",
    "\n",
    "- Weight Decay\n",
    "- Data Augmentation\n",
    "- Early Stopping on Validation Set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Decay\n",
    "\n",
    "Weight decay is a technique to reduce model complexity by adding a penalty to the magnitude of the weights. It can be implemented by decaying the weights towards 0 after each gradient descent step. \n",
    "\n",
    "Read the following documentation and add Weight Decay to your model: [torch.optim.Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam)\n",
    "\n",
    "It is implemented in the optimizer.\n",
    "\n",
    "Make it configurable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f06fb3ce8a5f489b999e3e186a6885bf",
     "grade": true,
     "grade_id": "cell-1df2c55234f1bd22",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torchmetrics\n",
    "\n",
    "\n",
    "class Classifier(pl.LightningModule):\n",
    "    def __init__(self, model, weight_decay=0):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.accuracy = torchmetrics.Accuracy(task=\"binary\")\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.loss_fn(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "        # Update accuracy metric\n",
    "        acc = self.accuracy(preds, y)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        self.log(\"train_acc\", acc, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "\n",
    "Data augmentation is the process of applying random transformations to the input data before it is processed by the model. This increases the robustness of the model and improves its generalization capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "\n",
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomResizedCrop((128, 128)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((128, 128)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Example usage with ImageDataset class\n",
    "ds_train = ImageDataset(train_image_paths, train_labels, transform=train_transform)\n",
    "dataloader_train = DataLoader(ds_train, batch_size=64, shuffle=True)\n",
    "\n",
    "# Create model instance\n",
    "model = Classifier(net)\n",
    "\n",
    "# Create a logger\n",
    "logger = TensorBoardLogger(\n",
    "    DATA_PATH.joinpath(\"lightning_logs\"), name=\"data_augmentation\"\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = pl.Trainer(\n",
    "    devices=1,\n",
    "    accelerator=\"cpu\",\n",
    "    precision=\"32\",\n",
    "    max_steps=100,\n",
    "    fast_dev_run=DEV_RUNS,\n",
    "    enable_checkpointing=False,\n",
    "    logger=logger,\n",
    "    default_root_dir=DATA_PATH.joinpath(\"lightning_logs\"),\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model, train_dataloaders=dataloader_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping\n",
    "\n",
    "Early stopping monitors the training process on a separate validation set to determine the optimal point regarding when to stop training (when validation loss / metric is at the best level).\n",
    "\n",
    "Pytorch-lightning provides such functionality out-of-the-box: [pytorch_lightning.callbacks.early_stopping.EarlyStopping](https://lightning.ai/docs/pytorch/stable/common/early_stopping.html)\n",
    "\n",
    "**Make sure to let the model run enough steps such that early stopping is actually stopping the training!**\n",
    "\n",
    "Implement a metric which early stopping should monitor. It should be one calculated on the validation set.\n",
    "\n",
    "\n",
    "Inspect the `Trainer` class and set more appropriate values  (e.g. `val_check_interval` and `max_steps`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ef8bbcfa38bc2f224c43167d29e85465",
     "grade": true,
     "grade_id": "cell-1f63d38b919c30a9",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "\n",
    "class Classifier(pl.LightningModule):\n",
    "    def __init__(self, model, weight_decay=0):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.train_accuracy = torchmetrics.Accuracy(task=\"binary\")\n",
    "        self.validation_accuracy = torchmetrics.Accuracy(task=\"binary\")\n",
    "        self.test_accuracy = torchmetrics.Accuracy(task=\"binary\")\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.loss_fn(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "        # Update accuracy metric\n",
    "        acc = self.train_accuracy(preds, y)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        self.log(\"train_acc\", acc, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(\n",
    "            self.parameters(), lr=0.001, weight_decay=self.weight_decay\n",
    "        )\n",
    "\n",
    "\n",
    "# Define early stopping callback\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"validation_acc\", min_delta=0.00, patience=3, mode=\"max\", verbose=True\n",
    ")\n",
    "\n",
    "# Create model instance\n",
    "model = Classifier(net, weight_decay=1e-4)\n",
    "\n",
    "# Datasets\n",
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomResizedCrop((128, 128)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((128, 128)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "ds_train = ImageDataset(train_image_paths, train_labels, transform=train_transform)\n",
    "ds_val = ImageDataset(\n",
    "    validation_image_paths, validation_labels, transform=val_transform\n",
    ")\n",
    "\n",
    "dataloader_train = DataLoader(ds_train, batch_size=64, shuffle=True)\n",
    "dataloader_val = DataLoader(ds_val, batch_size=64, shuffle=False)\n",
    "\n",
    "# Create a logger\n",
    "logger = TensorBoardLogger(DATA_PATH.joinpath(\"lightning_logs\"), name=\"early_stopping\")\n",
    "\n",
    "# Create trainer\n",
    "trainer = pl.Trainer(\n",
    "    devices=1,\n",
    "    accelerator=\"cpu\",\n",
    "    precision=\"32\",\n",
    "    max_steps=100,\n",
    "    fast_dev_run=DEV_RUNS,\n",
    "    enable_checkpointing=False,\n",
    "    logger=logger,\n",
    "    callbacks=[early_stopping],  # Add the early stopping callback here\n",
    "    default_root_dir=DATA_PATH.joinpath(\"lightning_logs\"),\n",
    ")\n",
    "\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model, train_dataloaders=dataloader_train, val_dataloaders=dataloader_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Compare Training metrics with validation metrics. What do you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2cb1bb623cb10dcfd4bae07066f33338",
     "grade": true,
     "grade_id": "cell-8e6eea84e21ace20",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5 - Hyper-Parameter Optimization\n",
    "\n",
    "To optimize hyper parameters we need to consider the following:\n",
    "- paramaterize training process (architecture and pre-processing)\n",
    "- experiment tracking software\n",
    "- evaluation procedures (such as cross-validation for smaller datasets)\n",
    "\n",
    "\n",
    "**Hyper-Parameter Tuning can be time consuming!**\n",
    "\n",
    "Ideally one uses special libraries such as [RayTune](https://docs.ray.io/en/latest/tune/index.html).\n",
    "\n",
    "Here you need to use a validation set or need to perform cross-validaton. Experiment tracking software such as tensorboard or weights & biases are highly recommended.\n",
    "\n",
    "You can implement a hyper-opt loop if you like. You could test different `weight_decay` values, different model architectures, or different data augmentation techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4c08a8761214b6ab1c94923363a5837b",
     "grade": true,
     "grade_id": "cell-1bb9b324c9f0b92d",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6 - Squeeze out the Juice!\n",
    "\n",
    "You can try the following techniques to get even further:\n",
    "\n",
    "- advanced data augmentation. For example: https://pytorch.org/vision/main/auto_examples/transforms/plot_cutmix_mixup.html#sphx-glr-auto-examples-transforms-plot-cutmix-mixup-py\n",
    "- model ensembling. Train multiple models and combine their predictions.\n",
    "- advanced techniques: AdamW Optimizer, Stochastic Depth Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f001d44aab8a91ac0e6b02cc7e00e5c8",
     "grade": true,
     "grade_id": "cell-3a2c866db0bc1afd",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c1a67d7790dac18f8c4a6c02a6aa4a53",
     "grade": false,
     "grade_id": "cell-52424cc5813975bf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# Evaluate your model\n",
    "\n",
    "We may want to evaluate our model in more detail. In particular we want to know where the model works well and where it fails. This might give us additional insight in the data and the difficulties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data loaders\n",
    "ds_test = ImageDataset(test_image_paths, test_labels, transform=val_transform)\n",
    "dataloader_test = DataLoader(ds_test, batch_size=64, shuffle=False)\n",
    "\n",
    "trainer.test(model, dataloaders=dataloader_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "39541001b33970f34a3c2a2ce0b794b5",
     "grade": false,
     "grade_id": "cell-3368f29ef9e4cc98",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### Confusion-Matrix\n",
    "\n",
    "Plotten Sie eine _confusion matrix_. Benutzen Sie \n",
    "\n",
    "- [confusion_matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)\n",
    "- [ConfusionMatrixDisplay](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html#sklearn.metrics.ConfusionMatrixDisplay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "13fe067c325f44324a28d6af3a687ab7",
     "grade": true,
     "grade_id": "cell-78b9e4f3c69747d5",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Which classes are confused how?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
