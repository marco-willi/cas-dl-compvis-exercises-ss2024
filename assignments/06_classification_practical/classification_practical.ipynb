{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CAS Deep Learning - Computer Vision mit Deep Learning (Part 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f10f19697935a18db955753fdc25539f",
     "grade": false,
     "grade_id": "header",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "480b39d39224b412ab147cd443698c7c",
     "grade": false,
     "grade_id": "title",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# Image Classification - Project\n",
    "\n",
    "## Learning Goals\n",
    "\n",
    "- Learn how to model an image classification task\n",
    "- Learn how to systematically implement and check each step from data prep to model selection and evaluation\n",
    "- Learn how to incorporate libraries which provide boilerplate code such as [torchvision](https://pytorch.org/vision/0.9/index.html) and  [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0b3592875de63679346c48263ac7ec03",
     "grade": false,
     "grade_id": "cell-a9856bbf98ea570f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Setup\n",
    "\n",
    "We setup our environment and data save / load paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bce791b7343c4ea54a5e764d57d57d7f",
     "grade": false,
     "grade_id": "imports",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e43ad09da2664c93d9494ef703961b80",
     "grade": false,
     "grade_id": "cell-964e127b4b508779",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Mount your google drive to store data and results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9a7a09d3adf20b74231fda01be8637cc",
     "grade": false,
     "grade_id": "cell-d573733e6ec05a88",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "print(f\"In colab: {IN_COLAB}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the following paths if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    DATA_PATH = Path(\"/content/drive/MyDrive/cas-dl-module-compvis-part1\")\n",
    "else:\n",
    "    DATA_PATH = Path(\"../data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install packages not in base Colab environment and required by you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    os.system(\"pip install torchshow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eebf2e294fc05ba8ca56da96b8f6d14a",
     "grade": false,
     "grade_id": "projects",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Project Selection\n",
    "\n",
    "Choose one of the following projects to work on. \n",
    "\n",
    "Choose Cats vs Dogs if you want to profit from example code and want to tag along.\n",
    "\n",
    "### Cats vs Dogs\n",
    "\n",
    "**Goal**: Develop a model to classify images of cats and dogs. The dataset is designed to facilitate the identification of these animals from images.\n",
    "\n",
    "**Approach**: Create a Convolutional Neural Network (CNN) to classify the images into two categories: cats and dogs. Experiment with various CNN architectures and techniques to determine the most effective method. Use data augmentation techniques to handle variations in pose, lighting, and background. Ensure the model generalizes well by using cross-validation and monitoring for overfitting.\n",
    "\n",
    "**Dataset**: The dataset contains 25,000 images, with approximately 12,500 images per class (cats and dogs). Each image varies in size and resolution. The data is provided by Microsoft as part of their Kaggle competition.\n",
    "\n",
    "[Source](https://www.microsoft.com/en-us/download/details.aspx?id=54765)\n",
    "\n",
    "![Dog](dog.jpg)\n",
    "![Cat](cat.jpg)\n",
    "\n",
    "\n",
    "### Concrete Crack Detection\n",
    "\n",
    "**Goal**: Develop a model to classify concrete images as having cracks or not. The dataset is designed to facilitate the identification of structural issues in concrete buildings.\n",
    "\n",
    "**Approach**: Create a Convolutional Neural Network (CNN) to classify the images into negative (no crack) and positive (crack) categories. Experiment with various CNN architectures and techniques to determine the most effective method. Use image processing techniques to handle variations in surface finish and illumination. Ensure the model generalizes well by using cross-validation and monitoring for overfitting.\n",
    "\n",
    "**Dataset**: The dataset contains 40,000 images, with 20,000 images per class (negative and positive). Each image is 227 x 227 pixels with RGB channels. The data is collected from 458 high-resolution images (4032 x 3024 pixels) from various METU Campus Buildings. No data augmentation such as random rotation or flipping is applied.\n",
    "\n",
    "[Source](https://data.mendeley.com/datasets/5y9wdsg2zt/2)\n",
    "\n",
    "![Crack](crack_example.jpg)\n",
    "![No Crack](crack_negative.jpg)\n",
    "\n",
    "\n",
    "### Scene Classification\n",
    "\n",
    "**Goal**: Develop a model to classify natural scene images into one of six categories. The dataset aims to facilitate the recognition of various natural scenes from around the world.\n",
    "\n",
    "**Approach**: Design a Convolutional Neural Network (CNN) to classify images into six categories: buildings, forest, glacier, mountain, sea, and street. Test different CNN architectures to find the best performing model. Apply data augmentation techniques to improve generalization. Separate the data into training, testing, and prediction sets to evaluate model performance effectively.\n",
    "\n",
    "**Dataset**: The dataset contains around 25,000 images of size 150 x 150 pixels, distributed across six categories. The data is separated into training (14,000 images), testing (3,000 images), and prediction (7,000 images) sets.\n",
    "\n",
    "[Source](https://www.kaggle.com/datasets/puneet6060/intel-image-classification?resource=download)\n",
    "\n",
    "\n",
    "![Builings](natural_scenes_buildings.jpg)\n",
    "![Forest](natural_scenes_forest.jpg)\n",
    "![Glacier](natural_scenes_glacier.jpg)\n",
    "\n",
    "\n",
    "\n",
    "### Satellite Land Cover Classification\n",
    "\n",
    "**Goal**: Develop a model to classify satellite images into different land cover types. The dataset contains images of 10 different classes and aims to support land use and land cover classification tasks.\n",
    "\n",
    "**Approach**: Develop Convolutional Neural Networks (CNNs) to model the satellite image data. Experiment with different CNN architectures to identify the best performing model. Compare pre-trained models with those trained from scratch. Use data augmentation techniques to enhance model generalization. Given the relatively small dataset, pay attention to overfitting and compare models robustly.\n",
    "\n",
    "**Dataset**: The dataset consists of 27,000 RGB images categorized into 10 classes. The dataset is available in two formats: one in RGB and another with 13 spectral bands. Use the RGB dataset for this project.\n",
    "\n",
    "[Source](https://github.com/phelber/eurosat)\n",
    "\n",
    "![Crop](sat_crop.jpg)\n",
    "![Forest](sat_forest.jpg)\n",
    "![Highway](sat_highway.jpg)\n",
    "\n",
    "\n",
    "### Choose your own dataset!\n",
    "\n",
    "Feel free to choose your own dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overall Approach\n",
    "\n",
    "Inspired by [A Recipe for Training Neural Networks by Andrej Karpathy](https://karpathy.github.io/2019/04/25/recipe/)\n",
    "\n",
    "For your chosen dataset. Do the following:\n",
    "\n",
    "\n",
    "## 1) Data\n",
    "\n",
    "- Download the data\n",
    "- Inspect the data formats\n",
    "- Build a `torch.utils.data.Dataset`\n",
    "    - define training, validation and test sets\n",
    "- Implement a `torch.utils.data.DataLoader'\n",
    "- Inspect the data:\n",
    "    - Look at samples\n",
    "    - Inspect the label distribution\n",
    "\n",
    "## 2) Baselines\n",
    "\n",
    "- Implement a small CNN\n",
    "- Learn input-independent baseline (provide only labels but random noise as input)\n",
    "- Overfitt CNN on one batch\n",
    "- Inspect pre-processing\n",
    "  \n",
    "## 3) (Over)fit\n",
    "- Build a large(er) architecture (pre-trained or self-implemented)\n",
    "- Train a high-performing model with respect to training set\n",
    "  \n",
    "## 4) Regularize\n",
    "- Is it beneficial to collect more data?\n",
    "- Data Augmentation\n",
    "- Early Stopping on Validation Set\n",
    "- Weight Decay\n",
    "\n",
    "## 5) Hyper-Parameter Tuning\n",
    "- Define HPs and parameterise architecture\n",
    "- do grid- or random search over HP grids\n",
    "\n",
    "## 6) Squeeze out the juice\n",
    "-  Ensembling\n",
    "-  Longer training\n",
    "-  Special techniques: AdamW optimizer, fancy data augmentation, label smoothing, stochastic depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Data\n",
    "\n",
    "- Download the data\n",
    "- Inspect the data formats and file organization\n",
    "- Remove corrupt data\n",
    "- Build a `torch.utils.data.Dataset`\n",
    "    - define training, validation and test sets\n",
    "- Implement a `torch.utils.data.DataLoader'\n",
    "- Inspect the data:\n",
    "    - Look at samples\n",
    "    - Inspect the label distribution\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the Data\n",
    "\n",
    "The following code snippets help you get the data quickly. It may take a while to download though..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_extract_zip(url: str, save_path: Path, extract_path: Path):\n",
    "    import os\n",
    "    import requests\n",
    "    import zipfile\n",
    "    \n",
    "    # Make sure the directory exists\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    \n",
    "    if not save_path.exists():\n",
    "        # Download the file\n",
    "        response = requests.get(url, stream=True)\n",
    "        with open(save_path, 'wb') as file:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                _ = file.write(chunk)\n",
    "    \n",
    "        print(f\"File downloaded and saved to {save_path}\")\n",
    "    \n",
    "    if not extract_path.exists():\n",
    "        # Unzip the file\n",
    "        with zipfile.ZipFile(save_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_path)\n",
    "        \n",
    "        print(f\"File extracted to {extract_path}\")\n",
    "\n",
    "\n",
    "def download_from_gdrive_and_extract_zip(file_id: str, save_path: Path, extract_path: Path):\n",
    "    import os\n",
    "    import requests\n",
    "    import gdown\n",
    "\n",
    "    url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "    if not save_path.exists():\n",
    "        gdown.download(url, str(save_path), quiet=False)\n",
    "        print(f\"File downloaded and saved to {save_path}\")\n",
    "    \n",
    "    if not extract_path.exists():\n",
    "        # Unzip the file\n",
    "        with zipfile.ZipFile(save_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_path)\n",
    "        \n",
    "        print(f\"File extracted to {extract_path}\")\n",
    "\n",
    "\n",
    "def delete_bad_file(file_path: Path):\n",
    "    # Check if file exists before trying to delete it\n",
    "    if os.path.exists(file_path):\n",
    "        os.remove(file_path)\n",
    "        print(f\"{file_path} has been deleted\")\n",
    "    else:\n",
    "        print(f\"{file_path} does not exist\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cats vs Dogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_and_extract_zip(\n",
    "    url = \"https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_5340.zip\",\n",
    "    save_path = DATA_PATH.joinpath(\"cats_vs_dogs.zip\"),\n",
    "    extract_path = DATA_PATH.joinpath(\"cats_vs_dogs/\")\n",
    ")\n",
    "\n",
    "bad_files = [\n",
    "    DATA_PATH.joinpath(\"cats_vs_dogs\") / \"PetImages\" / \"Cat\" / \"666.jpg\",\n",
    "    DATA_PATH.joinpath(\"cats_vs_dogs\") / \"PetImages\" / \"Dog\" / \"11702.jpg\"\n",
    "]\n",
    "\n",
    "\n",
    "for bad_file in bad_files:\n",
    "    delete_bad_file(bad_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Case you chose EuroSat Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_and_extract_zip(\n",
    "    url = \"https://zenodo.org/records/7711810/files/EuroSAT_RGB.zip?download=1\",\n",
    "    save_path = DATA_PATH.joinpath(\"EuroSAT_RGB.zip\"),\n",
    "    extract_path = DATA_PATH.joinpath(\"EuroSAT_RGB/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concrete Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_and_extract_zip(\n",
    "    url = \"https://prod-dcd-datasets-cache-zipfiles.s3.eu-west-1.amazonaws.com/5y9wdsg2zt-2.zip\",\n",
    "    save_path = DATA_PATH.joinpath(\"concrete.zip\"),\n",
    "    extract_path = DATA_PATH.joinpath(\"concrete/\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scene classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_from_gdrive_and_extract_zip(\n",
    "    file_id = \"1Bx3R56VBONS-x91wCDU6KX3xqPoJoH9P\",\n",
    "    save_path = DATA_PATH / \"scene_classification.zip\",\n",
    "    extract_path = DATA_PATH.joinpath(\"scene_classification/\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Data Format & Organization, Build Dataset and Loader\n",
    "\n",
    "We need to figure out how the data is organized. Particularly, how the data is labelled, to correctly define it with a `torch.utils.data.Dataset`.\n",
    "\n",
    "First you should look at the data / folder structure of the downloaded data.\n",
    "\n",
    "Once you have figured out how the data is organized we can build a `Dataset`. A `Dataset` allows for iterating over a dataset while returning tuples of images and labels.\n",
    "\n",
    "**We already create a training, validation and a test dataset.**\n",
    "\n",
    "Adapt the following code if necessary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Tuple\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, Subset\n",
    "\n",
    "\n",
    "def load_image_paths_and_labels(image_dir: str) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Load image paths and corresponding labels.\n",
    "\n",
    "    Args:\n",
    "        image_dir: Directory with all the images.\n",
    "\n",
    "    Returns:\n",
    "        A tuple of (image paths, labels).\n",
    "    \"\"\"\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    classes = os.listdir(image_dir)\n",
    "    image_extensions = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".gif\"}\n",
    "\n",
    "    for label in classes:\n",
    "        class_dir = os.path.join(image_dir, label)\n",
    "        for img_name in os.listdir(class_dir):\n",
    "            img_path = os.path.join(class_dir, img_name)\n",
    "            if any(img_path.lower().endswith(ext) for ext in image_extensions):\n",
    "                image_paths.append(img_path)\n",
    "                labels.append(label)\n",
    "\n",
    "    return image_paths, labels\n",
    "\n",
    "\n",
    "def create_train_test_split(\n",
    "        image_paths: List[str],\n",
    "        labels: List[str], \n",
    "        test_size: float=0.2,\n",
    "        random_state: int=None) -> Tuple[List[str], List[str], List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Create stratified train and test splits.\n",
    "\n",
    "    Args:\n",
    "        image_paths: List of image paths.\n",
    "        labels: List of labels.\n",
    "        test_size: The proportion of the dataset to include in the test split.\n",
    "        random_state: Controls the shuffling applied to the data before applying the split.\n",
    "\n",
    "    Returns:\n",
    "        train_image_paths, test_image_paths, train_labels, test_labels\n",
    "    \"\"\"\n",
    "    train_image_paths, test_image_paths, train_labels, test_labels = train_test_split(\n",
    "        image_paths, labels, stratify=labels, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    return train_image_paths, test_image_paths, train_labels, test_labels\n",
    "\n",
    "\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, image_paths: List[str], labels: List[str], transform: Callable | None=None, classes: List[str] = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_paths: List of image paths.\n",
    "            labels: List of labels.\n",
    "            transform: Optional transform to be applied on a sample.\n",
    "            classes: List of class names.\n",
    "        \"\"\"\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.classes = classes if classes is not None else sorted(set(labels))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            idx: Index\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (image, label) where label is the image classification.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            image_path = self.image_paths[idx]\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            label = self.labels[idx]\n",
    "            label_num = self.classes.index(label)\n",
    "\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image, label_num\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image at index {idx}: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "image_root_path = DATA_PATH.joinpath(\"cats_vs_dogs/PetImages\")\n",
    "image_paths, labels = load_image_paths_and_labels(image_root_path)\n",
    "\n",
    "# Create Train, Validation and Test Splits\n",
    "train_image_paths, test_image_paths, train_labels, test_labels = create_train_test_split(image_paths, labels, test_size=0.2, random_state=123)\n",
    "train_image_paths, validation_image_paths, train_labels, validation_labels = create_train_test_split(train_image_paths, train_labels, test_size=0.1, random_state=123)\n",
    "\n",
    "# Specify transformations\n",
    "train_transform = None  \n",
    "test_transform = None \n",
    "validation_transform = None\n",
    "\n",
    "ds_train = ImageDataset(train_image_paths, train_labels, transform=train_transform)\n",
    "ds_validation = ImageDataset(validation_image_paths, validation_labels, transform=validation_transform)\n",
    "ds_test = ImageDataset(test_image_paths, test_labels, transform=test_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e9a31c67793a09d3ff8e40f8dde5f1b3",
     "grade": false,
     "grade_id": "cell-974feaab57d71ef1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question**: What is the role of: `label_num = self.classes.index(label)`?\n",
    "\n",
    "**Question**: Why do we (often)  need a training, validation and a testset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e2ed81eb45dcde76dd79b913c96b9677",
     "grade": true,
     "grade_id": "cell-3df732b70f3385a9",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we take a look at an example from the `Dataset` to test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchshow as ts\n",
    "image, label = ds_train[0]\n",
    "ts.show(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For model training we need to batch examples. Thats why we need to define a `torch.utils.data.DataLoader`. \n",
    "\n",
    "We also need to convert the images to Tensors. We can use the `transform` parameter to specify transformations using `torchvision.transforms`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "torch.manual_seed(123)  \n",
    "\n",
    "# Define a simple transformation\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "\n",
    "])\n",
    "\n",
    "# Create the dataset and dataloader\n",
    "ds_train = ImageDataset(train_image_paths, train_labels, transform=train_transform)\n",
    "dataloader_train = DataLoader(ds_train, batch_size=16, shuffle=True)\n",
    "\n",
    "images, labels = next(iter(dataloader_train))\n",
    "\n",
    "ts.show(images)\n",
    "labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "de0aaa83c9c889e124fa64e85bcf3406",
     "grade": false,
     "grade_id": "cell-8385aee74294e0fc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question**: What does `shuffle=True` achieve? Why is it recommended?\n",
    "\n",
    "**Question**: Why do we use `torch.manual_seed(123)`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d366c2247b08b2559894f8891a32638d",
     "grade": true,
     "grade_id": "cell-b27f3f9aa63b1694",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Samples\n",
    "\n",
    "Now you can use the `Dataset` or `DataLoader` objects to insepct the dataset. Look for the following:\n",
    "\n",
    "- what is the class distribution? See [numpy.unique](https://numpy.org/doc/stable/reference/generated/numpy.unique.html).\n",
    "- how difficult is the problem?\n",
    "- are there any obvious issues with the data?\n",
    "- are the labels accurate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fefa064fb8714b9f7c3f50c0e9a6ae8e",
     "grade": true,
     "grade_id": "cell-556797ba20290ab9",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Baselines\n",
    "\n",
    "- Implement a small CNN\n",
    "- fix random seed\n",
    "- Learn input-independent baseline (provide only labels but random noise as input)\n",
    "- Overfitt CNN on one batch\n",
    "- Inspect pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement a small CNN\n",
    "\n",
    "For example you could implement the following architecture.\n",
    "\n",
    "- Input Shape: (3, 32, 32)\n",
    "- Convolution: 16 Filters, Kernel-Size 5x5\n",
    "- Pooling: Stride 2, Kernel-Size 2\n",
    "- Convolution: 32 Filter, Kernel-Size 5x5\n",
    "- Global Average Pooling\n",
    "- FC: 2 neurons (number of classes)\n",
    "\n",
    "Use `ReLU` activation after each convolution.\n",
    "\n",
    "Define a class which inherits from `torch.nn.Module`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "59e05a723da5d06e7abd71ede5d80115",
     "grade": true,
     "grade_id": "cell-282219b1ddf201ab",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import torchinfo\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, (5, 5))\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 5)\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "net = Net()\n",
    "\n",
    "print(net)\n",
    "print(torchinfo.summary(net, input_size=(1, 3, 32, 32)))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a training Loop\n",
    "\n",
    "We use Pytorch-Lightning which greatly simplifys implementing boilerplate code such as  training loops.\n",
    "\n",
    "Tutorial here: https://lightning.ai/pages/community/tutorial/step-by-step-walk-through-of-pytorch-lightning/\n",
    "\n",
    "We also include additional metrics from [torchmetrics](https://lightning.ai/docs/torchmetrics/stable/) to easily log and calculate accuracy.  Adapt `task`if necessary!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torchmetrics\n",
    "\n",
    "class Classifier(pl.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.accuracy = torchmetrics.Accuracy(task=\"binary\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.loss_fn(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        \n",
    "        # Update accuracy metric\n",
    "        acc = self.accuracy(preds, y)\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        self.log('train_acc', acc, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the following parameters accoring to your hardware. As you can see, this simplifies hardware switches greatly!\n",
    "\n",
    "We want to perform a functional check only. Train the model only for 10 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(\n",
    "\tdevices=1,\n",
    "\taccelerator=\"cpu\",\n",
    "\tprecision=\"32\",\n",
    "    max_steps=10,\n",
    "    enable_checkpointing=False,\n",
    "    logger=False,\n",
    "    default_root_dir=DATA_PATH.joinpath(\"lightning_logs\")\n",
    "\t)\n",
    "\n",
    "net = Net()\n",
    "model = Classifier(net)\n",
    "trainer.fit(model, train_dataloaders=dataloader_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train the model for longer to get a sense of the performance. Adjust the following code accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(\n",
    "\tdevices=1,\n",
    "\taccelerator=\"cpu\",\n",
    "\tprecision=\"32\",\n",
    "    max_steps=1,\n",
    "    enable_checkpointing=False,\n",
    "    logger=False,\n",
    "    default_root_dir=DATA_PATH.joinpath(\"lightning_logs\")\n",
    "\t)\n",
    "\n",
    "net = Net()\n",
    "model = Classifier(net)\n",
    "trainer.fit(model, train_dataloaders=dataloader_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Metrics:  {trainer.logged_metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn Input independent Model\n",
    "\n",
    "\n",
    "Modify the `Dataset` class such that random images, e.g. white noise, is returned. The label remains unchanged. Then train a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d9b24a0d36f057b1d92e073cad417ba5",
     "grade": false,
     "grade_id": "cell-a1689e4300aaf490",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question**: What kind of loss do you expect if the model works?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "960afd90bd3959ab396f104d4fe37428",
     "grade": true,
     "grade_id": "cell-38d958f99b140ac7",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDatasetRandom(ImageDataset):\n",
    "    \n",
    "    def __getitem__(self, idx: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            idx: Index\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (image, label) where label is the image classification.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            image_path = self.image_paths[idx]\n",
    "            original_image = np.array(Image.open(image_path).convert(\"RGB\"))\n",
    "            image_shape = original_image.shape\n",
    "            random_image = np.random.randint(0, 256, image_shape, dtype=np.uint8)\n",
    "            \n",
    "            label = self.labels[idx]\n",
    "            label_num = self.classes.index(label)\n",
    "\n",
    "            if self.transform:\n",
    "                random_image = self.transform(random_image)\n",
    "            return random_image, label_num\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image at index {idx}: {e}\")\n",
    "            return None\n",
    "\n",
    "# Create the dataset and dataloader\n",
    "ds_train_random = ImageDatasetRandom(train_image_paths, train_labels, transform=train_transform)\n",
    "dataloader_random = DataLoader(ds_train_random, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify your work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_random, label = ds_random[0]\n",
    "ts.show(image_random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(\n",
    "\tdevices=1,\n",
    "\taccelerator=\"cpu\",\n",
    "\tprecision=\"32\",\n",
    "    max_steps=100,\n",
    "    enable_checkpointing=False,\n",
    "    logger=False,\n",
    "    default_root_dir=DATA_PATH.joinpath(\"lightning_logs\")\n",
    "\t)\n",
    "\n",
    "net = Net()\n",
    "model = Classifier(net)\n",
    "trainer.fit(model, train_dataloaders=dataloader_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Metrics:  {trainer.logged_metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfit on one Batch of Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "64f826f5e7bb804d539ce756c493a089",
     "grade": false,
     "grade_id": "cell-76e81732d77174f3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Question**: What do you expect?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9106e3bb32a105b5c70ffffc2e2ba7f6",
     "grade": true,
     "grade_id": "cell-52aedf9dae498678",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(\n",
    "\tdevices=1,\n",
    "\taccelerator=\"cpu\",\n",
    "\tprecision=\"32\",\n",
    "    max_steps=100,\n",
    "    enable_checkpointing=False,\n",
    "    logger=False,\n",
    "    default_root_dir=DATA_PATH.joinpath(\"lightning_logs\"),\n",
    "    limit_train_batches=1.0\n",
    "\t)\n",
    "\n",
    "net = Net()\n",
    "model = Classifier(net)\n",
    "\n",
    "ds_train = ImageDataset(train_image_paths, train_labels, transform=train_transform)\n",
    "dataloader_train = DataLoader(ds_train, batch_size=64, shuffle=False)\n",
    "trainer.fit(model, train_dataloaders=dataloader_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Metrics:  {trainer.logged_metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) (Over)Fit\n",
    "\n",
    "In this step we try to drive the trainings-loss as low as possible.\n",
    "\n",
    "You can do the following:\n",
    "- implement your own model\n",
    "- use a pre-defined model\n",
    "- use a pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1336a73cae07c21a44610fbda288504f",
     "grade": false,
     "grade_id": "cell-1f3c0de79c9f603b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## Pre-Trained Model\n",
    "\n",
    "In the following we will use a pre-trained model and adapt it to our dataset (transfer-learning).\n",
    "\n",
    "### Load Model\n",
    "\n",
    "Here we use a pre-trained model.  Read the doc here: [https://pytorch.org/vision/0.8/models.html](https://pytorch.org/vision/0.8/models.html).)\n",
    "\n",
    "**It is important to read how the data is pre-processed for a given pre-trained model. This should be consistent with how you pre-process the data.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d712c344f6689697736d6961f08e8078",
     "grade": true,
     "grade_id": "cell-39c22ca50688c3fe",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we adapt the output layer to match our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.fc = nn.Sequential(nn.Linear(512, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(\n",
    "\tdevices=1,\n",
    "\taccelerator=\"cpu\",\n",
    "\tprecision=\"32\",\n",
    "    max_steps=100,\n",
    "    enable_checkpointing=False,\n",
    "    logger=False,\n",
    "    default_root_dir=DATA_PATH.joinpath(\"lightning_logs\")\n",
    "\t)\n",
    "\n",
    "model = Classifier(net)\n",
    "\n",
    "ds_train = ImageDataset(train_image_paths, train_labels, transform=transforms['train'])\n",
    "dataloader_train = DataLoader(ds_train, batch_size=64, shuffle=False)\n",
    "trainer.fit(model, train_dataloaders=dataloader_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Metrics:  {trainer.logged_metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Regularization\n",
    "\n",
    "Regularization is a process to deliberately limit a model's capacity in order to reduce overfitting and to improve generalization.\n",
    "\n",
    "There are different techniques:\n",
    "\n",
    "- Weight Decay\n",
    "- Data Augmentation\n",
    "- Early Stopping on Validation Set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Decay\n",
    "\n",
    "Weight decay is a technique to reduce model complexity by adding a penalty to the magnitude of the weights. It can be implemented by decaying the weights towards 0 after each gradient descent step. \n",
    "\n",
    "Read the following documentation and add Weight Decay to your model: [torch.optim.Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam)\n",
    "\n",
    "It is implemented in the optimizer.\n",
    "\n",
    "Make it configurable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eec31d48bb3b3d49b719109aaaaf87ef",
     "grade": true,
     "grade_id": "cell-1df2c55234f1bd22",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torchmetrics\n",
    "\n",
    "class Classifier(pl.LightningModule):\n",
    "    def __init__(self, model, weight_decay=0):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.accuracy = torchmetrics.Accuracy(task=\"binary\")\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.loss_fn(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        \n",
    "        # Update accuracy metric\n",
    "        acc = self.accuracy(preds, y)\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        self.log('train_acc', acc, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "\n",
    "Data augmentation is the process of applying random transformations to the input data before it is processed by the model. This increases the robustness of the model and improves its generalization capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(128),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(128),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# Example usage with ImageDataset class\n",
    "ds_train = ImageDataset(train_image_paths, train_labels, transform=transforms['train'])\n",
    "ds_val = ImageDataset(val_image_paths, val_labels, transform=transforms['val'])\n",
    "\n",
    "dataloader_train = DataLoader(ds_train, batch_size=64, shuffle=True)\n",
    "dataloader_val = DataLoader(ds_val, batch_size=64, shuffle=False)\n",
    "\n",
    "# Create model instance\n",
    "model = Classifier(net)\n",
    "\n",
    "# Create trainer\n",
    "trainer = pl.Trainer(\n",
    "    devices=1,\n",
    "    accelerator=\"cpu\",\n",
    "    precision=\"32\",\n",
    "    max_steps=100,\n",
    "    enable_checkpointing=False,\n",
    "    logger=False,\n",
    "    callbacks=[early_stopping],  # Add the early stopping callback here\n",
    "    default_root_dir=DATA_PATH.joinpath(\"lightning_logs\")\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model, train_dataloaders=dataloader_train, val_dataloaders=dataloader_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping\n",
    "\n",
    "Early stopping monitors the training process on a separate validation set to determine the optimal point regarding when to stop training (when validation loss / metric is at the best level).\n",
    "\n",
    "Pytorch-lightning provides such functionality out-of-the-box: [pytorch_lightning.callbacks.early_stopping.EarlyStopping](https://lightning.ai/docs/pytorch/stable/common/early_stopping.html)\n",
    "\n",
    "**Make sure to let the model run enough steps such that early stopping is actually stopping the training!**\n",
    "\n",
    "Implement a metric which early stopping should monitor. It should be one calculated on the validation set.\n",
    "\n",
    "\n",
    "Inspect the `Trainer` class and set more appropriate values  (e.g. `val_check_interval` and `max_steps`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a7a3a8fd26d718b0807774247c991194",
     "grade": true,
     "grade_id": "cell-1f63d38b919c30a9",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "\n",
    "class Classifier(pl.LightningModule):\n",
    "    def __init__(self, model, weight_decay=0):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.train_accuracy = torchmetrics.Accuracy(task=\"binary\")\n",
    "        self.validation_accuracy = torchmetrics.Accuracy(task=\"binary\")\n",
    "        self.test_accuracy = torchmetrics.Accuracy(task=\"binary\")\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.loss_fn(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        \n",
    "        # Update accuracy metric\n",
    "        acc = self.train_accuracy(preds, y)\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        self.log('train_acc', acc, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.001, weight_decay=self.weight_decay)\n",
    "\n",
    "# Define early stopping callback\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='validation_acc', \n",
    "    min_delta=0.00,\n",
    "    patience=3, \n",
    "    mode='max',\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Create model instance\n",
    "model = Classifier(net, weight_decay=1e-4)\n",
    "\n",
    "# Prepare data loaders\n",
    "ds_train = ImageDataset(train_image_paths, train_labels, transform=transforms['train'])\n",
    "dataloader_train = DataLoader(ds_train, batch_size=64, shuffle=True)\n",
    "\n",
    "# Create trainer\n",
    "trainer = pl.Trainer(\n",
    "    devices=1,\n",
    "    accelerator=\"cpu\",\n",
    "    precision=\"32\",\n",
    "    max_steps=10,\n",
    "    val_check_interval=5, \n",
    "    enable_checkpointing=False,\n",
    "    logger=False,\n",
    "    callbacks=[early_stopping],  # Add the early stopping callback here\n",
    "    default_root_dir=DATA_PATH.joinpath(\"lightning_logs\")\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model, train_dataloaders=dataloader_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Hyper-Parameter Optimization\n",
    "\n",
    "To optimize hyper parameters we need to consider the following:\n",
    "- paramaterize training process (architecture and pre-processing)\n",
    "- experiment tracking software\n",
    "- evaluation procedures (such as cross-validation for smaller datasets)\n",
    "\n",
    "\n",
    "**Hyper-Parameter Tuning can be time consuming!**\n",
    "\n",
    "Ideally one uses special libraries such as [RayTune](https://docs.ray.io/en/latest/tune/index.html).\n",
    "\n",
    "You can implement a hyper-opt loop if you like. You could test different `weight_decay` values or different model architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4c08a8761214b6ab1c94923363a5837b",
     "grade": true,
     "grade_id": "cell-1bb9b324c9f0b92d",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6) Squeeze out the Juice!\n",
    "\n",
    "You can try the following techniques to get even further:\n",
    "\n",
    "- advanced data augmentation. For example: https://pytorch.org/vision/main/auto_examples/transforms/plot_cutmix_mixup.html#sphx-glr-auto-examples-transforms-plot-cutmix-mixup-py\n",
    "- model ensembling. Train multiple models and combine their predictions.\n",
    "- advanced techniques: AdamW Optimizer, Stochastic Depth Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f001d44aab8a91ac0e6b02cc7e00e5c8",
     "grade": true,
     "grade_id": "cell-3a2c866db0bc1afd",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c1a67d7790dac18f8c4a6c02a6aa4a53",
     "grade": false,
     "grade_id": "cell-52424cc5813975bf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# Evaluate your model\n",
    "\n",
    "We may want to evaluate our model in more detail. In particular we want to know where the model works well and where it fails. This might give us additional insight in the data and the difficulties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data loaders\n",
    "ds_test= ImageDataset(test_image_paths, test_labels, transform=transforms['val'])\n",
    "dataloader_test = DataLoader(ds_test, batch_size=64, shuffle=False)\n",
    "\n",
    "trainer.test(model, dataloaders=dataloader_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "39541001b33970f34a3c2a2ce0b794b5",
     "grade": false,
     "grade_id": "cell-3368f29ef9e4cc98",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "### Confusion-Matrix\n",
    "\n",
    "Plotten Sie eine _confusion matrix_. Benutzen Sie \n",
    "\n",
    "- [confusion_matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)\n",
    "- [ConfusionMatrixDisplay](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html#sklearn.metrics.ConfusionMatrixDisplay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "13fe067c325f44324a28d6af3a687ab7",
     "grade": true,
     "grade_id": "cell-78b9e4f3c69747d5",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Which classes are confused how?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
